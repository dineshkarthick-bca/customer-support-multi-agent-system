{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI Customer Support Multi-Agent System (RAG + Memory + A2A + API + Long-Running Tasks)\nEnd-to-End Enterprise-Grade Support Agent Built with Gemini, FAISS, Memory Bank & Multi-Agent Orchestration\n\nðŸ“ Project Summary\n\nThis project implements a production-grade Customer Support AI Agent using a fully modular multi-agent system, powered by:\n\nðŸ”¹ Gemini 2.5 Flash Lite (LLM generation)\n\nðŸ”¹ text-embedding-004 (vector embeddings)\n\nðŸ”¹ FAISS (semantic retrieval)\n\nðŸ”¹ Long-term memory bank (vector memory per user)\n\nðŸ”¹ Domain-specialized agents (payment, refund, order, account, technical support)\n\nðŸ”¹ A2A protocol (agent-to-agent messages and escalation)\n\nðŸ”¹ Orchestrator agent (routing, context compaction, memory + KB fusion)\n\nðŸ”¹ FastAPI REST API (full backend server)\n\nðŸ”¹ Long-running task engine (pause/resume refund investigation)\n\nðŸ”¹ Evaluation suite (intent accuracy, retrieval coverage, hallucination tests, latency)\n\nThe system replicates a real customer support backend that can:\n\nUnderstand user queries\n\nRetrieve knowledge from documents\n\nUse user-specific memories\n\nRoute the request to correct domain agent\n\nEscalate queries between agents\n\nExecute long-running background tasks\n\nServe responses through an API\n\nAll within a single Kaggle Notebook, without needing external deployment.\n\nðŸŽ¯ Problem Statement\n\nHandling customer support ticketsâ€”especially at scaleâ€”is slow, repetitive, and expensive.\nCustomers often wait for human agents to respond to questions such as:\n\nâ€œPayment deducted but no confirmationâ€\n\nâ€œWhere is my order?â€\n\nâ€œRefund not received yetâ€\n\nâ€œApp keeps crashingâ€\n\nâ€œHow do I delete my account?â€\n\nMost of these queries have structured answers, but humans still manually search FAQs or knowledge base articles.\n\nThis results in:\n\nHigh ticket resolution time\n\nInconsistent answers\n\nPoor customer satisfaction\n\nBottlenecks during peak seasons\n\nHigh operational cost\n\nWe need an automated, intelligent, reliable support system that can handle 70â€“80% of tickets without human intervention.\n\nðŸš€ Solution Overview\n\nThis project builds an end-to-end AI Customer Support System using the Multi-Agent architecture taught in the Google Agents Intensive program.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 2. Environment Setup & Required Installations\n\nIn this step, we prepare the Kaggle environment for the multi-agent customer support system.\nThis cell:\n\nInstalls all required Python libraries (Gemini API, FAISS, FastAPI, Sentence Transformers, etc.)\n\nCreates a clean project directory structure inside /kaggle/working\n\nEnsures folders for KB, embeddings, FAISS index, logs, uploads, and models\n\nConfirms successful setup by printing directory paths\n\nThis creates a stable foundation for building the Retrieval-Augmented Generation (RAG) pipeline, memory bank, agents, and API server.","metadata":{}},{"cell_type":"code","source":"# Cell 2 â€” Setup & Installs\n# Run once. On Kaggle this will be fine; on other environments ensure permissions.\n\n# Install required libs (comment/uncomment as needed)\n!pip install -q google-generativeai faiss-cpu sentence-transformers fastapi uvicorn nest_asyncio python-multipart\n\n# Imports\nimport os, time, json, uuid, threading, shutil\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport numpy as np\nimport pickle\nimport faiss\nfrom pprint import pprint\n\n# Configure project paths (Kaggle default)\nBASE_DIR = Path(\"/kaggle/working\")\nDATA_DIR = BASE_DIR / \"data\"\nKB_DIR = DATA_DIR / \"knowledge_base\"\nEMB_DIR = DATA_DIR / \"embeddings\"\nFAISS_DIR = DATA_DIR / \"faiss_index\"\nLOGS_DIR = DATA_DIR / \"logs\"\nUPLOADS_DIR = DATA_DIR / \"uploads\"\nMODELS_DIR = BASE_DIR / \"models\"\n\nfor d in [DATA_DIR, KB_DIR, EMB_DIR, FAISS_DIR, LOGS_DIR, UPLOADS_DIR, MODELS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nprint(\"Directories ensured under\", DATA_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:59:58.943640Z","iopub.execute_input":"2025-11-29T11:59:58.945117Z","iopub.status.idle":"2025-11-29T12:00:35.576690Z","shell.execute_reply.started":"2025-11-29T11:59:58.945031Z","shell.execute_reply":"2025-11-29T12:00:35.575416Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDirectories ensured under /kaggle/working/data\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 3. Configure Gemini API (LLM + Embeddings)\n\nThis cell initializes Googleâ€™s Generative AI client (google-generativeai) to enable:\n\nText generation (via Gemini 2.5 Flash Lite)\n\nEmbedding generation (via text-embedding-004)\n\nIt securely loads your API key:\n\nFirst from environment variables\n\nIf missing, from Kaggle Secrets under GENAI_API_KEY\n\nFinally, it defines:\n\nEMBED_MODEL = models/text-embedding-004\n\nLLM_MODEL = models/gemini-2.5-flash-lite (low quota usage)","metadata":{}},{"cell_type":"code","source":"# Cell 3 â€” Configure Gemini (google-generativeai)\nimport os\nimport google.generativeai as genai\n\n# Load API key from Kaggle Secrets\n# Make sure you created a secret called: GENAI_API_KEY\nGENAI_API_KEY = os.environ.get(\"GENAI_API_KEY\")\n\nif not GENAI_API_KEY:\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        GENAI_API_KEY = user_secrets.get_secret(\"GENAI_API_KEY\")\n    except Exception as e:\n        raise ValueError(\"âŒ GENAI_API_KEY is missing. Add it in 'Add-ons â†’ Secrets'.\") from e\n\n# Configure the model\ngenai.configure(api_key=GENAI_API_KEY)\n\n# Default Models\nEMBED_MODEL = \"models/text-embedding-004\"\nLLM_MODEL = \"models/gemini-2.5-flash-lite\"\n\nprint(\"Using Embedding model:\", EMBED_MODEL)\nprint(\"Using LLM model:\", LLM_MODEL)\nprint(\"ðŸ” Gemini API Key loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:35.578914Z","iopub.execute_input":"2025-11-29T12:00:35.579364Z","iopub.status.idle":"2025-11-29T12:00:41.395240Z","shell.execute_reply.started":"2025-11-29T12:00:35.579323Z","shell.execute_reply":"2025-11-29T12:00:41.393786Z"}},"outputs":[{"name":"stdout","text":"Using Embedding model: models/text-embedding-004\nUsing LLM model: models/gemini-2.5-flash-lite\nðŸ” Gemini API Key loaded successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"4. Load Sample Knowledge Base (KB) & Sample Tickets\n\nIf this is the notebook's first run, this cell automatically creates:\n\nâœ” Sample KB documents\n\npayment_issues.txt\n\nrefund_policy.txt\n\norder_status.txt\n\napp_troubleshooting.txt\n\naccount_management.txt\n\nThese serve as seed knowledge for the RAG system.\n\nâœ” Sample support tickets\n\nStored in tickets_sample.json, used for evaluation and classifier training.\n\nThis ensures the full pipeline can run end-to-end even without uploading your own KB.","metadata":{}},{"cell_type":"code","source":"# Cell 4 â€” Provide sample KB documents and sample tickets if empty (so notebook runs)\nif not any(KB_DIR.glob(\"*\")):\n    samples = {\n        \"payment_issues.txt\": \"\"\"PAYMENT ISSUE TROUBLESHOOTING\n1. PAYMENT DEDUCTED BUT NO CONFIRMATION - Wait 5 minutes; if continues, check bank; refund processed automatically on failure.\n2. DOUBLE PAYMENT - One payment auto-refunds in 5-7 working days.\n3. UPI REFUNDS - 24-72 hours typical; banks vary.\"\"\",\n        \"order_status.txt\": \"\"\"ORDER STATUS INFORMATION\n1. Pending - Waiting for payment gateway confirmation.\n2. Confirmed - Email/SMS sent.\n3. Failed - Refund initiated automatically.\"\"\",\n        \"refund_policy.txt\": \"\"\"REFUND POLICY\nRefunds take 5-10 business days for card, 24-72 hours for UPI. Provide Transaction ID to investigate.\"\"\",\n        \"app_troubleshooting.txt\": \"\"\"APP TROUBLESHOOTING\n- Clear cache, update app, restart device, reinstall if needed.\"\"\",\n        \"account_management.txt\": \"\"\"ACCOUNT ISSUES\n- To delete account contact support with reason. Account deletion is irreversible.\"\"\"\n    }\n    for name, text in samples.items():\n        p = KB_DIR / name\n        p.write_text(text)\n    print(\"Sample KB created in\", KB_DIR)\n\n# sample tickets\nTICKETS_PATH = DATA_DIR / \"tickets_sample.json\"\nif not TICKETS_PATH.exists():\n    sample_tickets = [\n        {\"id\": \"t1\", \"user_message\": \"Payment deducted but no confirmation\"},\n        {\"id\": \"t2\", \"user_message\": \"Where is my order?\"},\n        {\"id\": \"t3\", \"user_message\": \"I didn't get my refund yet\"},\n        {\"id\": \"t4\", \"user_message\": \"App keeps crashing on open\"},\n        {\"id\": \"t5\", \"user_message\": \"How do I delete my account?\"}\n    ]\n    TICKETS_PATH.write_text(json.dumps(sample_tickets, indent=2))\n    print(\"Sample tickets saved:\", TICKETS_PATH)\n\nprint(\"KB files:\", list(KB_DIR.glob(\"*\")))\nprint(\"Tickets:\", TICKETS_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:41.396659Z","iopub.execute_input":"2025-11-29T12:00:41.397304Z","iopub.status.idle":"2025-11-29T12:00:41.409990Z","shell.execute_reply.started":"2025-11-29T12:00:41.397241Z","shell.execute_reply":"2025-11-29T12:00:41.408564Z"}},"outputs":[{"name":"stdout","text":"Sample KB created in /kaggle/working/data/knowledge_base\nSample tickets saved: /kaggle/working/data/tickets_sample.json\nKB files: [PosixPath('/kaggle/working/data/knowledge_base/refund_policy.txt'), PosixPath('/kaggle/working/data/knowledge_base/account_management.txt'), PosixPath('/kaggle/working/data/knowledge_base/app_troubleshooting.txt'), PosixPath('/kaggle/working/data/knowledge_base/payment_issues.txt'), PosixPath('/kaggle/working/data/knowledge_base/order_status.txt')]\nTickets: /kaggle/working/data/tickets_sample.json\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"5. Utility Functions: Cleaning + Chunking\n\nDefines helpful text preprocessing utilities:\n\nðŸ”¹ clean_text()\n\nRemoves newlines, tabs\n\nCompresses whitespace\n\nðŸ”¹ chunk_text()\n\nSplits long documents into small, retrievable chunks (max ~800 chars).\n\nThese utilities are crucial for:\n\n1.Building high-quality embeddings\n\n2.Improving RAG retrieval accuracy","metadata":{}},{"cell_type":"code","source":"# Cell 5 â€” Utilities\nimport re\ndef clean_text(t: str) -> str:\n    if not isinstance(t, str):\n        return \"\"\n    t = t.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef chunk_text(text: str, max_chars: int = 800):\n    parts = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    out = []\n    for p in parts:\n        if len(p) <= max_chars:\n            out.append(p)\n        else:\n            for i in range(0, len(p), max_chars):\n                out.append(p[i:i+max_chars])\n    return out\n\nprint(\"Utilities ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:41.411337Z","iopub.execute_input":"2025-11-29T12:00:41.412534Z","iopub.status.idle":"2025-11-29T12:00:43.870776Z","shell.execute_reply.started":"2025-11-29T12:00:41.412495Z","shell.execute_reply":"2025-11-29T12:00:43.869386Z"}},"outputs":[{"name":"stdout","text":"Utilities ready\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"6. Build Text Embeddings for RAG\n\nThis step:\n\nLoads and chunks KB documents\n\nPrepares sample tickets\n\nCombines all text passages\n\nGenerates embeddings using Gemini:\n\ngenai.embed_content() for each text\n\nSaves:\n\nembeddings_gemini.npy\n\nmeta_gemini.pkl\n\nIf files already exist, they are reloaded.\n\nThis forms the core vector database for RAG-based answers.","metadata":{}},{"cell_type":"code","source":"# Cell 6 â€” Build embeddings (RAG) and metadata\nfrom tqdm.auto import tqdm\n\n# Load KB docs\nkb_docs = []\nfor p in sorted(KB_DIR.glob(\"*\")):\n    if p.suffix.lower() in [\".txt\", \".md\"]:\n        kb_docs.append({\"name\": p.name, \"text\": p.read_text(encoding=\"utf-8\")})\n\n# Build passages\nkb_passages = []\nfor doc in kb_docs:\n    chunks = chunk_text(clean_text(doc[\"text\"]))\n    for i, c in enumerate(chunks):\n        kb_passages.append({\"id\": f\"{doc['name']}_chunk_{i}\", \"source\": doc['name'], \"text\": c})\n\nticket_examples = json.loads(TICKETS_PATH.read_text(encoding=\"utf-8\"))\n\nticket_texts = [t[\"user_message\"] for t in ticket_examples]\nall_texts = [p[\"text\"] for p in kb_passages] + ticket_texts\n\nprint(f\"KB passages: {len(kb_passages)}, tickets: {len(ticket_examples)}, total texts: {len(all_texts)}\")\n\n# Embeddings (use genai.embed_content in batches)\ndef embed_texts(texts: List[str]) -> np.ndarray:\n    embeds = []\n    for t in tqdm(texts):\n        try:\n            resp = genai.embed_content(model=EMBED_MODEL, content=t, task_type=\"retrieval_document\")\n            v = np.array(resp[\"embedding\"], dtype=\"float32\")\n        except Exception as e:\n            print(\"Embedding error (falling back to random):\", e)\n            # fallback random vector (not ideal, but keeps notebook running)\n            v = np.random.rand(768).astype(\"float32\")\n        embeds.append(v)\n    return np.vstack(embeds).astype(\"float32\")\n\nemb_path = EMB_DIR / \"embeddings_gemini.npy\"\nmeta_path = EMB_DIR / \"meta_gemini.pkl\"\n\nif not emb_path.exists() or not meta_path.exists():\n    embeddings = embed_texts(all_texts)\n    np.save(emb_path, embeddings)\n    meta = {\n        \"kb_passages\": kb_passages,\n        \"tickets\": ticket_examples,\n        \"texts\": all_texts,\n        \"kb_count\": len(kb_passages),\n        \"embedding_model\": EMBED_MODEL,\n        \"dim\": embeddings.shape[1]\n    }\n    with open(meta_path, \"wb\") as f:\n        pickle.dump(meta, f)\n    print(\"Saved embeddings and meta\")\nelse:\n    embeddings = np.load(emb_path)\n    with open(meta_path, \"rb\") as f:\n        meta = pickle.load(f)\n    print(\"Loaded existing embeddings & meta from disk\")\n\nprint(\"Embeddings shape:\", embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:43.874670Z","iopub.execute_input":"2025-11-29T12:00:43.875036Z","iopub.status.idle":"2025-11-29T12:00:45.704937Z","shell.execute_reply.started":"2025-11-29T12:00:43.875007Z","shell.execute_reply":"2025-11-29T12:00:45.703831Z"}},"outputs":[{"name":"stdout","text":"KB passages: 5, tickets: 5, total texts: 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45bde43c02054904b9450f3e128a2683"}},"metadata":{}},{"name":"stdout","text":"Saved embeddings and meta\nEmbeddings shape: (10, 768)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"7. Create FAISS Vector Index\n\nHere we:\n\nNormalize embeddings with L2 norm\n\nBuild a FAISS IndexFlatIP (inner product search)\n\nSave it to disk\n\nLoad metadata for retrieval later\n\nThis index powers fast semantic search over your knowledge base.","metadata":{}},{"cell_type":"code","source":"# Cell 7 â€” Build FAISS index\nemb = embeddings\ndim = emb.shape[1]\nfaiss.normalize_L2(emb)\n\nindex_path = FAISS_DIR / \"kb_faiss_gemini.index\"\nif not index_path.exists():\n    index = faiss.IndexFlatIP(dim)\n    index.add(emb)\n    faiss.write_index(index, str(index_path))\n    shutil.copy(meta_path, FAISS_DIR / \"meta_gemini.pkl\")\n    print(\"FAISS index built and saved:\", index_path)\nelse:\n    index = faiss.read_index(str(index_path))\n    print(\"FAISS index loaded:\", index_path)\n\n# load texts\ntexts = meta.get(\"texts\", [])\nprint(\"Number of texts:\", len(texts))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.706310Z","iopub.execute_input":"2025-11-29T12:00:45.706573Z","iopub.status.idle":"2025-11-29T12:00:45.715170Z","shell.execute_reply.started":"2025-11-29T12:00:45.706553Z","shell.execute_reply":"2025-11-29T12:00:45.714145Z"}},"outputs":[{"name":"stdout","text":"FAISS index built and saved: /kaggle/working/data/faiss_index/kb_faiss_gemini.index\nNumber of texts: 10\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"8. Define Retriever Functions\n\nThis cell enables semantic search:\n\nðŸ”¹ embed_query()\n\nCreates an embedding vector for user queries.\n\nðŸ”¹ retrieve(query, k)\n\nReturns top-k most relevant KB chunks using FAISS.\n\nThis will be used by all domain agents to access contextual knowledge.","metadata":{}},{"cell_type":"code","source":"# Cell 8 â€” Retriever (RAG)\ndef embed_query(query: str):\n    try:\n        resp = genai.embed_content(model=EMBED_MODEL, content=query, task_type=\"retrieval_query\")\n        v = np.array(resp[\"embedding\"], dtype=\"float32\").reshape(1, -1)\n    except Exception as e:\n        print(\"Embed query error:\", e)\n        v = np.random.rand(1, dim).astype(\"float32\")\n    faiss.normalize_L2(v)\n    return v\n\ndef retrieve(query: str, k: int = 4):\n    v = embed_query(query)\n    try:\n        scores, ids = index.search(v, k)\n    except Exception as e:\n        print(\"FAISS search error, returning empty:\", e)\n        return []\n    results = []\n    for idx, score in zip(ids[0], scores[0]):\n        if idx < 0 or idx >= len(texts):\n            continue\n        results.append({\"score\": float(score), \"text\": texts[idx]})\n    return results\n\n# quick test\nprint(\"Retriever test:\", retrieve(\"payment deducted but no confirmation\", 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.716531Z","iopub.execute_input":"2025-11-29T12:00:45.717386Z","iopub.status.idle":"2025-11-29T12:00:45.884848Z","shell.execute_reply.started":"2025-11-29T12:00:45.717319Z","shell.execute_reply":"2025-11-29T12:00:45.883843Z"}},"outputs":[{"name":"stdout","text":"Retriever test: [{'score': 0.7178919315338135, 'text': 'Payment deducted but no confirmation'}, {'score': 0.6268824934959412, 'text': 'PAYMENT ISSUE TROUBLESHOOTING 1. PAYMENT DEDUCTED BUT NO CONFIRMATION - Wait 5 minutes; if continues, check bank; refund processed automatically on failure. 2. DOUBLE PAYMENT - One payment auto-refunds in 5-7 working days. 3. UPI REFUNDS - 24-72 hours typical; banks vary.'}]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"9. Long-Term Memory Bank (Vector Store)\n\nImplements a persistent memory system enabling the agent to:\n\nStore summaries of past user tickets\n\nRetrieve memories per user\n\nMaintain context across multiple conversations\n\nMemory stored using:\n\nFAISS index for fast similarity search\n\nmemory_meta.pkl for metadata\n\nFunctions included:\n\ninit_memory_bank()\n\nstore_memory()\n\nretrieve_memories()\n\nThis satisfies the Sessions & Long-term Memory requirement.","metadata":{}},{"cell_type":"code","source":"# Cell 9 â€” Memory Bank (persistent FAISS + metadata)\nMEM_DIR = DATA_DIR / \"memory_bank\"\nMEM_DIR.mkdir(parents=True, exist_ok=True)\nMEM_INDEX_PATH = MEM_DIR / \"memory_faiss.index\"\nMEM_META_PATH = MEM_DIR / \"memory_meta.pkl\"\n\n_memory_index = None\n_memory_meta = None\n\ndef _save_meta():\n    with open(MEM_META_PATH, \"wb\") as f:\n        pickle.dump(_memory_meta, f)\n\ndef _save_index():\n    faiss.write_index(_memory_index, str(MEM_INDEX_PATH))\n\ndef init_memory_bank(dim_mb: int = 768):\n    global _memory_index, _memory_meta\n    if MEM_META_PATH.exists() and MEM_INDEX_PATH.exists():\n        with open(MEM_META_PATH, \"rb\") as f:\n            _memory_meta = pickle.load(f)\n        _memory_index = faiss.read_index(str(MEM_INDEX_PATH))\n        print(\"Memory Bank loaded:\", len(_memory_meta.get(\"memories\", {})), \"memories.\")\n    else:\n        from collections import defaultdict\n        _memory_meta = {\"next_id\": 0, \"memories\": {}, \"user_index\": defaultdict(list), \"dim\": dim_mb}\n        _memory_index = faiss.IndexFlatIP(dim_mb)\n        print(\"Memory Bank initialized (empty).\")\n    return True\n\ndef _embed_text_mem(text: str):\n    try:\n        resp = genai.embed_content(model=EMBED_MODEL, content=text, task_type=\"retrieval_document\")\n        v = np.array(resp[\"embedding\"], dtype=\"float32\").reshape(1, -1)\n    except Exception as e:\n        print(\"Memory embed fallback:\", e)\n        v = np.random.rand(1, 768).astype(\"float32\")\n    faiss.normalize_L2(v)\n    return v\n\ndef store_memory(user_id: str, text: str, metadata: dict = None):\n    global _memory_index, _memory_meta\n    if metadata is None:\n        metadata = {}\n    if _memory_index is None:\n        init_memory_bank()\n    v = _embed_text_mem(text)\n    _memory_index.add(v)\n    mem_id = int(_memory_meta[\"next_id\"])\n    ts = int(time.time())\n    _memory_meta[\"memories\"][mem_id] = {\"user_id\": user_id, \"text\": text, \"metadata\": metadata, \"ts\": ts}\n    _memory_meta[\"user_index\"].setdefault(user_id, []).append(mem_id)\n    _memory_meta[\"next_id\"] += 1\n    _save_meta()\n    _save_index()\n    return mem_id\n\ndef retrieve_memories(user_id: str, query: str = None, k: int = 5):\n    global _memory_index, _memory_meta\n    if _memory_index is None:\n        init_memory_bank()\n    if user_id not in _memory_meta[\"user_index\"] or len(_memory_meta[\"user_index\"][user_id]) == 0:\n        return []\n    if query is None:\n        mem_ids = _memory_meta[\"user_index\"].get(user_id, [])\n        mems = sorted([(mid, _memory_meta[\"memories\"][mid]) for mid in mem_ids], key=lambda x: x[1][\"ts\"], reverse=True)[:k]\n        return [{\"mem_id\": mid, \"text\": data[\"text\"], \"metadata\": data[\"metadata\"], \"ts\": data[\"ts\"], \"score\": None} for mid, data in mems]\n    qv = _embed_text_mem(query)\n    D, I = _memory_index.search(qv, min(64, max(1, _memory_meta[\"next_id\"])))\n    out = []\n    for score, idx in zip(D[0], I[0]):\n        if idx < 0:\n            continue\n        mem_meta = _memory_meta[\"memories\"].get(int(idx))\n        if mem_meta and mem_meta[\"user_id\"] == user_id:\n            out.append({\"mem_id\": int(idx), \"text\": mem_meta[\"text\"], \"metadata\": mem_meta[\"metadata\"], \"ts\": mem_meta[\"ts\"], \"score\": float(score)})\n        if len(out) >= k:\n            break\n    return out\n\n# init memory bank\ninit_memory_bank()\nprint(\"Memory bank ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.886276Z","iopub.execute_input":"2025-11-29T12:00:45.886590Z","iopub.status.idle":"2025-11-29T12:00:45.908919Z","shell.execute_reply.started":"2025-11-29T12:00:45.886567Z","shell.execute_reply":"2025-11-29T12:00:45.907182Z"}},"outputs":[{"name":"stdout","text":"Memory Bank initialized (empty).\nMemory bank ready\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"10. Rule-Based Intent Classifier\n\nA lightweight intent routing system:\n\nMatches keywords against 6 intents:\n\nPAYMENT_ISSUE\n\nORDER_STATUS\n\nREFUND\n\nACCOUNT_ISSUE\n\nTECH_SUPPORT\n\nGENERAL_QUERY\n\nReturns:\n\nDetected intent\n\nAssigned agent\n\nConfidence score\n\nThis determines which domain agent should process the query.","metadata":{}},{"cell_type":"code","source":"# Cell 10 â€” Intent classifier (rules + optional LLM refine)\nINTENT_KEYWORDS = {\n    \"PAYMENT_ISSUE\": [\"payment\", \"pay\", \"deducted\", \"upi\", \"refund\"],\n    \"ORDER_STATUS\": [\"order\", \"shipping\", \"delivered\", \"where is my order\"],\n    \"REFUND\": [\"refund\", \"refunded\", \"money back\"],\n    \"ACCOUNT_ISSUE\": [\"delete account\", \"account\", \"login\", \"password\"],\n    \"TECH_SUPPORT\": [\"crash\", \"error\", \"app crash\", \"bug\"],\n    \"GENERAL_QUERY\": [\"return\", \"policy\", \"help\", \"support\"]\n}\n\ndef route_query(text: str) -> dict:\n    t = text.lower()\n    for intent, kws in INTENT_KEYWORDS.items():\n        for kw in kws:\n            if kw in t:\n                # Choose agent mapping\n                agent_map = {\n                    \"PAYMENT_ISSUE\": \"PAYMENT_AGENT\",\n                    \"ORDER_STATUS\": \"ORDER_AGENT\",\n                    \"REFUND\": \"REFUND_AGENT\",\n                    \"ACCOUNT_ISSUE\": \"ACCOUNT_AGENT\",\n                    \"TECH_SUPPORT\": \"TECH_AGENT\",\n                    \"GENERAL_QUERY\": \"GENERAL_AGENT\"\n                }\n                return {\"intent\": intent, \"agent\": agent_map.get(intent, \"GENERAL_AGENT\"), \"confidence\": 0.9, \"user_query\": text}\n    # fallback - label general, low confidence\n    return {\"intent\": \"GENERAL_QUERY\", \"agent\": \"GENERAL_AGENT\", \"confidence\": 0.3, \"user_query\": text}\n\nprint(\"route_query running examples:\")\nprint(route_query(\"Payment deducted but no confirmation\"))\nprint(route_query(\"How to delete my account?\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.910403Z","iopub.execute_input":"2025-11-29T12:00:45.910679Z","iopub.status.idle":"2025-11-29T12:00:45.937319Z","shell.execute_reply.started":"2025-11-29T12:00:45.910659Z","shell.execute_reply":"2025-11-29T12:00:45.936499Z"}},"outputs":[{"name":"stdout","text":"route_query running examples:\n{'intent': 'PAYMENT_ISSUE', 'agent': 'PAYMENT_AGENT', 'confidence': 0.9, 'user_query': 'Payment deducted but no confirmation'}\n{'intent': 'ACCOUNT_ISSUE', 'agent': 'ACCOUNT_AGENT', 'confidence': 0.9, 'user_query': 'How to delete my account?'}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"11. Agent-to-Agent (A2A) Message Helpers\n\nDefines utility wrappers for multi-agent communication:\n\nðŸ”¹ a2a_message(target, content)\n\nRequests escalation to another agent.\n\nðŸ”¹ final_answer(text)\n\nMarks completion of an agentâ€™s work.\n\nThis is part of the A2A protocol required for multi-agent systems.","metadata":{}},{"cell_type":"code","source":"# Cell 11 â€” A2A helpers\ndef a2a_message(target: str, content: str) -> dict:\n    return {\"type\": \"a2a_message\", \"target\": target, \"content\": content}\n\ndef final_answer(text: str) -> dict:\n    return {\"type\": \"final_answer\", \"content\": text}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.938251Z","iopub.execute_input":"2025-11-29T12:00:45.938566Z","iopub.status.idle":"2025-11-29T12:00:45.958636Z","shell.execute_reply.started":"2025-11-29T12:00:45.938545Z","shell.execute_reply":"2025-11-29T12:00:45.957298Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"12. Domain-Specific Agents (LLM-Powered)\n\nImplements 6 domain agents:\n\nPAYMENT_AGENT\n\nREFUND_AGENT\n\nORDER_AGENT\n\nTECH_AGENT\n\nACCOUNT_AGENT\n\nGENERAL_AGENT\n\nFeatures:\n\nâœ” RAG-Augmented LLM\n\nUses retrieved KB chunks + user memories + the query.\n\nâœ” Escalation Logic\n\nE.g., Payment â†’ Refund\n\nâœ” Robust fallback answers\n\nIn case of LLM quota or errors.\n\nThese are the core LLM-powered workers of the system.","metadata":{}},{"cell_type":"code","source":"# Cell 12 â€” Domain Agents (use LLM to generate final answers; they may return A2A messages)\ndef generate_agent_answer(user_query: str, domain: str) -> str:\n    # Build context from RAG + memory\n    # Ensure retrieve returns list of dicts -> extract texts\n    rag_chunks = retrieve(user_query, k=4)\n    chunk_texts = [c[\"text\"] if isinstance(c, dict) else c for c in rag_chunks]\n    mems = retrieve_memories(user_id=\"demo_user\", query=user_query, k=3)  # placeholder user, real orchestrator will pass user_id\n    mem_texts = [m[\"text\"] for m in mems]\n    mem_block = \"\\n\".join(f\"- {t}\" for t in mem_texts) if mem_texts else \"\"\n\n    context = \"\\n\\n---\\n\\n\".join(chunk_texts)\n    prompt = f\"\"\"\nYou are the {domain} of a Customer Support system.\nUse ONLY the CONTEXT below to answer, and if insufficient, ask a concise follow-up.\n\nCONTEXT:\n{context}\n\nUSER QUERY:\n{user_query}\n\nUSER MEMORIES:\n{mem_block}\n\nRespond concisely.\n\"\"\"\n    try:\n        model = genai.GenerativeModel(LLM_MODEL)\n        resp = model.generate_content(prompt)\n        text = resp.text if hasattr(resp, \"text\") else str(resp)\n    except Exception as e:\n        print(\"LLM call failed in generate_agent_answer:\", e)\n        # fallback: rule-based short answers\n        if \"payment\" in user_query.lower():\n            text = \"Please wait 5 minutes. If the order fails, refund will be processed automatically.\"\n        elif \"refund\" in user_query.lower():\n            text = \"Please provide Transaction ID and screenshot of debit message to investigate.\"\n        elif \"order\" in user_query.lower():\n            text = \"Your order is pending payment confirmation; provide order id to check.\"\n        elif \"crash\" in user_query.lower():\n            text = \"Try clearing cache, reinstalling the app, and restarting your device.\"\n        elif \"delete account\" in user_query.lower():\n            text = \"Account deletion is irreversible. Confirm if you wish to proceed.\"\n        else:\n            text = \"I do not have enough context; please provide more details.\"\n    return text\n\n# Domain-specific wrappers (may produce A2A if they detect escalation)\ndef PAYMENT_AGENT(query: str):\n    q = query.lower()\n    if \"refund\" in q or \"refunded\" in q or \"didn't receive my refund\" in q:\n        return a2a_message(\"REFUND_AGENT\", \"Payment agent detected refund request.\")\n    ans = generate_agent_answer(query, \"PAYMENT_AGENT\")\n    return final_answer(ans)\n\ndef REFUND_AGENT(query: str):\n    ans = generate_agent_answer(query, \"REFUND_AGENT\")\n    return final_answer(ans)\n\ndef ORDER_AGENT(query: str):\n    ans = generate_agent_answer(query, \"ORDER_AGENT\")\n    return final_answer(ans)\n\ndef TECH_AGENT(query: str):\n    ans = generate_agent_answer(query, \"TECH_AGENT\")\n    return final_answer(ans)\n\ndef ACCOUNT_AGENT(query: str):\n    ans = generate_agent_answer(query, \"ACCOUNT_AGENT\")\n    return final_answer(ans)\n\ndef GENERAL_AGENT(query: str):\n    ans = generate_agent_answer(query, \"GENERAL_AGENT\")\n    return final_answer(ans)\n\n# Agent function map for orchestrator\nAGENT_FN_MAP = {\n    \"PAYMENT_AGENT\": PAYMENT_AGENT,\n    \"REFUND_AGENT\": REFUND_AGENT,\n    \"ORDER_AGENT\": ORDER_AGENT,\n    \"TECH_AGENT\": TECH_AGENT,\n    \"ACCOUNT_AGENT\": ACCOUNT_AGENT,\n    \"GENERAL_AGENT\": GENERAL_AGENT,\n    # \"ESCALATION_AGENT\": ... (optional)\n}\n\nprint(\"Domain agents defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.959587Z","iopub.execute_input":"2025-11-29T12:00:45.959914Z","iopub.status.idle":"2025-11-29T12:00:45.976019Z","shell.execute_reply.started":"2025-11-29T12:00:45.959890Z","shell.execute_reply":"2025-11-29T12:00:45.974801Z"}},"outputs":[{"name":"stdout","text":"Domain agents defined\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"13. Multi-Agent Orchestrator (A2A Loop + Memory + Logging)\n\nThis is the heart of the system.\n\nThe orchestrator:\n\nðŸ”¥ Routes query â†’ correct agent\nðŸ”¥ Executes agent â†’ handles A2A escalations\nðŸ”¥ Retrieves memories (context engineering)\nðŸ”¥ Retrieves RAG chunks (compact context)\nðŸ”¥ Logs full ticket trace\nðŸ”¥ Stores summary into Memory Bank\nðŸ”¥ Returns final consolidated response\n\nSupports up to 6 hops of inter-agent messaging.\n\nThis fulfills:\n\nMulti-agent system\n\nContext compaction\n\nMemory\n\nA2A protocol\n\nObservability (ticket logs)","metadata":{}},{"cell_type":"code","source":"# Cell 13 â€” Orchestrator (A2A loop + memory + compaction + ticket logging)\ndef compact_context_from_retriever(rag_items: List[Dict[str,Any]]):\n    # simple compaction: pick top 4 texts and join\n    texts_list = [it[\"text\"] if isinstance(it, dict) else it for it in rag_items][:4]\n    return \"RELEVANT KNOWLEDGE:\\n\" + \"\\n\\n---\\n\\n\".join(texts_list)\n\nTICKET_LOG_PATH = LOGS_DIR / \"tickets_log.json\"\nif not TICKET_LOG_PATH.exists():\n    TICKET_LOG_PATH.write_text(\"[]\", encoding=\"utf-8\")\n\ndef _append_ticket(ticket: dict):\n    data = json.loads(TICKET_LOG_PATH.read_text(encoding=\"utf-8\"))\n    data.append(ticket)\n    TICKET_LOG_PATH.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n\ndef process_user_query(user_query: str, user_id: str = None, attachments: List[str] = None, debug: bool = False, max_hops: int = 6):\n    if attachments is None: attachments = []\n    ts = int(time.time())\n    user_id = user_id or f\"user_{ts}\"\n\n    # Route\n    routing = route_query(user_query)\n    intent = routing.get(\"intent\", \"GENERAL_QUERY\")\n    current_agent = routing.get(\"agent\", \"GENERAL_AGENT\")\n    confidence = routing.get(\"confidence\", 0.0)\n\n    # memory retrieval\n    memories = retrieve_memories(user_id, query=user_query, k=3)\n    mem_block = \"\\n\".join(f\"- {m['text']}\" for m in memories) if memories else \"\"\n\n    # RAG chunks\n    rag_chunks = retrieve(user_query, k=4)\n    compact_context = compact_context_from_retriever(rag_chunks)\n\n    # build agent input\n    agent_input = user_query\n    if attachments:\n        agent_input += \"\\n\\nAttachments: \" + \", \".join(attachments)\n    if mem_block:\n        agent_input = f\"{mem_block}\\n\\n{agent_input}\"\n\n    # A2A loop\n    hop = 0\n    a2a_trace = []\n    last_agent = current_agent\n    agent_fn = AGENT_FN_MAP.get(current_agent)\n    if agent_fn is None:\n        agent_fn = lambda q: final_answer(\"Sorry, no agent available.\")\n\n    final_message = None\n    try:\n        raw = agent_fn(agent_input)\n        # normalize\n        if isinstance(raw, dict) and raw.get(\"type\") == \"a2a_message\":\n            msg = raw\n        else:\n            # ensure dict final_answer\n            if isinstance(raw, dict) and raw.get(\"type\")==\"final_answer\":\n                msg = raw\n            else:\n                msg = final_answer(raw if not isinstance(raw, dict) else str(raw))\n        # loop\n        while msg.get(\"type\") == \"a2a_message\" and hop < max_hops:\n            hop += 1\n            target = msg[\"target\"]\n            content = msg[\"content\"]\n            a2a_trace.append({\"hop\": hop, \"from\": last_agent, \"to\": target, \"content\": content})\n            target_fn = AGENT_FN_MAP.get(target, lambda q: final_answer(\"Target agent missing\"))\n            raw = target_fn(content)\n            if isinstance(raw, dict) and raw.get(\"type\")==\"a2a_message\":\n                msg = raw\n            else:\n                if isinstance(raw, dict) and raw.get(\"type\")==\"final_answer\":\n                    msg = raw\n                else:\n                    msg = final_answer(raw if not isinstance(raw, dict) else str(raw))\n            last_agent = target\n        final_message = msg\n    except Exception as e:\n        if debug: print(\"Orchestrator agent error:\", e)\n        final_message = final_answer(\"Internal orchestration error.\")\n\n    final_answer_text = final_message.get(\"content\") if isinstance(final_message, dict) else str(final_message)\n\n    # Summarize (simple summary)\n    summary_text = json.dumps({\"summary\": f\"{intent}: {user_query}\", \"intent\": intent, \"agent\": last_agent})\n\n    # persist ticket\n    ticket_id = str(uuid.uuid4())\n    ticket_record = {\n        \"ticket_id\": ticket_id,\n        \"timestamp\": ts,\n        \"user_id\": user_id,\n        \"user_query\": user_query,\n        \"intent\": intent,\n        \"initial_agent\": current_agent,\n        \"final_agent\": last_agent,\n        \"confidence\": float(confidence),\n        \"answer\": final_answer_text,\n        \"summary\": summary_text,\n        \"attachments\": attachments,\n        \"a2a_trace\": a2a_trace,\n        \"compact_context\": compact_context\n    }\n    _append_ticket(ticket_record)\n\n    # store memory (store summary)\n    try:\n        store_memory(user_id, text=summary_text, metadata={\"ticket_id\": ticket_id})\n    except Exception:\n        pass\n\n    return {\n        \"ticket_id\": ticket_id,\n        \"intent\": intent,\n        \"initial_agent\": current_agent,\n        \"final_agent\": last_agent,\n        \"confidence\": float(confidence),\n        \"answer\": final_answer_text,\n        \"summary\": summary_text,\n        \"a2a_trace\": a2a_trace,\n        \"memory_used\": memories,\n        \"compact_context\": compact_context,\n        \"persisted_to\": str(TICKET_LOG_PATH)\n    }\n\nprint(\"Orchestrator ready. Try: process_user_query('Payment deducted but no confirmation', user_id='demo')\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:45.977235Z","iopub.execute_input":"2025-11-29T12:00:45.977588Z","iopub.status.idle":"2025-11-29T12:00:46.007569Z","shell.execute_reply.started":"2025-11-29T12:00:45.977565Z","shell.execute_reply":"2025-11-29T12:00:46.006591Z"}},"outputs":[{"name":"stdout","text":"Orchestrator ready. Try: process_user_query('Payment deducted but no confirmation', user_id='demo')\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"14. Long-Running Task Engine (Async Simulation)\n\nImplements a realistic long operation:\n\nRefund investigation (0â€“100% progress)\n\nSupports:\n\nstart_refund_investigation()\n\nget_task_status()\n\npause_task()\n\nresume_task()\n\nStored in a JSON-backed task store.\n\nFulfills pause/resume long-running agents requirement.","metadata":{}},{"cell_type":"code","source":"# Cell 14 â€” Long-running Task Manager (simple thread-based)\nTASKS_PATH = LOGS_DIR / \"long_tasks.json\"\nif not TASKS_PATH.exists():\n    TASKS_PATH.write_text(\"{}\", encoding=\"utf-8\")\n\n_task_store = {}\n_task_lock = threading.Lock()\n\ndef _save_tasks_to_disk():\n    with _task_lock:\n        TASKS_PATH.write_text(json.dumps(_task_store, indent=2), encoding=\"utf-8\")\n\ndef start_refund_investigation(user_id: str, order_id: str, description: str = \"\"):\n    job_id = str(uuid.uuid4())\n    job = {\"job_id\": job_id, \"user_id\": user_id, \"order_id\": order_id, \"description\": description, \"status\": \"running\", \"progress\": 0, \"start_time\": int(time.time())}\n    _task_store[job_id] = job\n    _save_tasks_to_disk()\n\n    def _worker(jid):\n        for p in range(0, 101, 10):\n            time.sleep(0.8)\n            if _task_store[jid][\"status\"] == \"paused\":\n                while _task_store[jid][\"status\"] == \"paused\":\n                    time.sleep(0.5)\n            _task_store[jid][\"progress\"] = p\n            _save_tasks_to_disk()\n        _task_store[jid][\"status\"] = \"done\"\n        _save_tasks_to_disk()\n\n    th = threading.Thread(target=_worker, args=(job_id,), daemon=True)\n    th.start()\n    return job\n\ndef get_task_status(job_id: str):\n    return _task_store.get(job_id)\n\ndef pause_task(job_id: str):\n    if job_id in _task_store:\n        _task_store[job_id][\"status\"] = \"paused\"\n        _save_tasks_to_disk()\n        return _task_store[job_id]\n    return None\n\ndef resume_task(job_id: str):\n    if job_id in _task_store:\n        _task_store[job_id][\"status\"] = \"running\"\n        _save_tasks_to_disk()\n        return _task_store[job_id]\n    return None\n\nprint(\"Long-running Task engine ready (start_refund_investigation, get_task_status, pause_task, resume_task)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:46.008908Z","iopub.execute_input":"2025-11-29T12:00:46.009150Z","iopub.status.idle":"2025-11-29T12:00:46.032236Z","shell.execute_reply.started":"2025-11-29T12:00:46.009131Z","shell.execute_reply":"2025-11-29T12:00:46.031163Z"}},"outputs":[{"name":"stdout","text":"Long-running Task engine ready (start_refund_investigation, get_task_status, pause_task, resume_task)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"15. FastAPI REST API for Multi-Agent System\n\nThis cell exposes the entire system over HTTP:\n\nEndpoints:\n\n/ask â†’ Ask main agent\n\n/start_refund_task â†’ Begin long-running job\n\n/task_status/{id} â†’ Get job progress\n\n/pause_task â†’ Pause job\n\n/resume_task â†’ Resume job\n\nRuns uvicorn in background\n\nSo FastAPI works inside Kaggle.\n\nThis enables:\n\nClient applications\n\nMobile apps\n\nFrontends\n\nExternal evaluation\n\nYou also satisfy Tooling + Deployment readiness","metadata":{}},{"cell_type":"code","source":"# Cell 15 â€” FastAPI REST API (run in notebook; uses thread to run uvicorn)\nfrom fastapi import FastAPI, UploadFile, File, Form\nfrom fastapi.responses import JSONResponse\nimport nest_asyncio, uvicorn\n\nnest_asyncio.apply()\n\napp = FastAPI(title=\"Customer Support Multi-Agent API\")\n\n@app.post(\"/ask\")\nasync def ask(user_query: str = Form(...), user_id: str = Form(\"anonymous\"), file: UploadFile = File(None)):\n    attachments = []\n    if file is not None:\n        save_path = UPLOADS_DIR / f\"{uuid.uuid4()}_{file.filename}\"\n        with open(save_path, \"wb\") as f:\n            f.write(await file.read())\n        attachments.append(str(save_path))\n    # always include nothing else; call orchestrator\n    try:\n        result = process_user_query(user_query, user_id=user_id, attachments=attachments)\n    except Exception as e:\n        return JSONResponse({\"error\": str(e)}, status_code=500)\n    return JSONResponse(result)\n\n@app.post(\"/start_refund_task\")\nasync def start_refund_task(user_id: str = Form(...), order_id: str = Form(...), description: str = Form(\"\")):\n    job = start_refund_investigation(user_id=user_id, order_id=order_id, description=description)\n    return JSONResponse(job)\n\n@app.get(\"/task_status/{job_id}\")\nasync def task_status(job_id: str):\n    job = get_task_status(job_id)\n    if job is None:\n        return JSONResponse({\"error\": \"not found\"}, status_code=404)\n    return JSONResponse(job)\n\n@app.post(\"/pause_task\")\nasync def pause_job(job_id: str = Form(...)):\n    job = pause_task(job_id)\n    if job is None:\n        return JSONResponse({\"error\": \"not found\"}, status_code=404)\n    return JSONResponse(job)\n\n@app.post(\"/resume_task\")\nasync def resume_job(job_id: str = Form(...)):\n    job = resume_task(job_id)\n    if job is None:\n        return JSONResponse({\"error\": \"not found\"}, status_code=404)\n    return JSONResponse(job)\n\n# Start server in background thread (safe for notebooks)\ndef _run_uvicorn():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nserver_thread = threading.Thread(target=_run_uvicorn, daemon=True)\nserver_thread.start()\nprint(\"FastAPI started on port 8000 in background thread. Test with curl -X POST -F 'user_query=Hi' http://127.0.0.1:8000/ask\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:46.035258Z","iopub.execute_input":"2025-11-29T12:00:46.035575Z","iopub.status.idle":"2025-11-29T12:00:47.020254Z","shell.execute_reply.started":"2025-11-29T12:00:46.035552Z","shell.execute_reply":"2025-11-29T12:00:47.019067Z"}},"outputs":[{"name":"stdout","text":"FastAPI started on port 8000 in background thread. Test with curl -X POST -F 'user_query=Hi' http://127.0.0.1:8000/ask\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"16. Quick Sanity Tests\n\nThis verifies:\n\nâœ” Orchestrator is functioning\nâœ” RAG retrieval works\nâœ” Agents produce valid answers\nâœ” Long-running tasks simulate correctly\n\nEssential for debugging before running evaluation.","metadata":{}},{"cell_type":"code","source":"# Cell 16 â€” Quick tests (local)\nprint(\"Orchestrator quick test:\")\nprint(process_user_query(\"Payment deducted but no confirmation\", user_id=\"demo_user\"))\n\n# Start a long-running refund task\njob = start_refund_investigation(\"user123\", \"ORD9988\", \"Investigating refund for order ORD9988\")\nprint(\"Started job:\", job[\"job_id\"])\ntime.sleep(1)\nprint(\"Status sample:\", get_task_status(job[\"job_id\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:47.021971Z","iopub.execute_input":"2025-11-29T12:00:47.022236Z","iopub.status.idle":"2025-11-29T12:00:49.037980Z","shell.execute_reply.started":"2025-11-29T12:00:47.022216Z","shell.execute_reply":"2025-11-29T12:00:49.036674Z"}},"outputs":[{"name":"stderr","text":"INFO:     Started server process [47]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"Orchestrator quick test:\n{'ticket_id': 'e6ce3c0f-f04e-4067-ba44-0515b4e0a430', 'intent': 'PAYMENT_ISSUE', 'initial_agent': 'PAYMENT_AGENT', 'final_agent': 'PAYMENT_AGENT', 'confidence': 0.9, 'answer': \"Please wait 5 minutes. If you still haven't received confirmation, check your bank. If the payment failed, a refund will be processed automatically.\", 'summary': '{\"summary\": \"PAYMENT_ISSUE: Payment deducted but no confirmation\", \"intent\": \"PAYMENT_ISSUE\", \"agent\": \"PAYMENT_AGENT\"}', 'a2a_trace': [], 'memory_used': [], 'compact_context': \"RELEVANT KNOWLEDGE:\\nPayment deducted but no confirmation\\n\\n---\\n\\nPAYMENT ISSUE TROUBLESHOOTING 1. PAYMENT DEDUCTED BUT NO CONFIRMATION - Wait 5 minutes; if continues, check bank; refund processed automatically on failure. 2. DOUBLE PAYMENT - One payment auto-refunds in 5-7 working days. 3. UPI REFUNDS - 24-72 hours typical; banks vary.\\n\\n---\\n\\nORDER STATUS INFORMATION 1. Pending - Waiting for payment gateway confirmation. 2. Confirmed - Email/SMS sent. 3. Failed - Refund initiated automatically.\\n\\n---\\n\\nI didn't get my refund yet\", 'persisted_to': '/kaggle/working/data/logs/tickets_log.json'}\nStarted job: 2f19d7df-6be7-43c2-9b43-00fed813af62\nStatus sample: {'job_id': '2f19d7df-6be7-43c2-9b43-00fed813af62', 'user_id': 'user123', 'order_id': 'ORD9988', 'description': 'Investigating refund for order ORD9988', 'status': 'running', 'progress': 0, 'start_time': 1764417648}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"17. Full Agent Evaluation Suite\n\nRuns comprehensive performance scoring:\n\nðŸ§ª Tests include:\n\nIntent classifier accuracy\n\nRAG retriever coverage\n\nDomain agent answer quality\n\nA2A routing correctness\n\nHallucination safety\n\nEnd-to-end latency\n\nðŸ“Š Produces final report:\n\nIntent Accuracy\n\nRAG Coverage\n\nAgent Quality\n\nA2A correctness\n\nHallucination Score\n\nE2E Latency\n\nThis completes the Agent Evaluation requirement.","metadata":{}},{"cell_type":"code","source":"# Cell 17 â€” Evaluation Suite (Intent accuracy, RAG coverage, Agent quality, A2A, Hallucination, E2E Latency)\nimport time\nfrom statistics import mean\n\nprint(\"ðŸ“Œ Starting Agent Evaluation Suite...\\n\")\n\n# 1. Intent tests\nintent_tests = [\n    (\"Payment deducted but no confirmation\", \"PAYMENT_ISSUE\"),\n    (\"I want to delete my account\", \"ACCOUNT_ISSUE\"),\n    (\"Where is my order?\", \"ORDER_STATUS\"),\n    (\"App keeps crashing\", \"TECH_SUPPORT\"),\n    (\"I didnâ€™t receive my refund\", \"REFUND\"),\n]\nintent_results = []\nfor text, expected in intent_tests:\n    out = route_query(text)\n    predicted = out[\"intent\"]\n    intent_results.append(predicted == expected)\n    print(f\"Test: {text} â†’ predicted={predicted}, expected={expected}\")\nintent_accuracy = sum(intent_results) / len(intent_results)\nprint(\"\\nðŸŽ¯ Intent Classifier Accuracy:\", intent_accuracy)\n\n# 2. RAG coverage\nrag_tests = [\"payment deducted\", \"refund delayed\", \"order pending\", \"app crash\"]\nrag_scores = []\nfor q in rag_tests:\n    chunks = retrieve(q, k=2)\n    score = 1 if len(chunks) > 0 else 0\n    rag_scores.append(score)\n    print(f\"RAG for '{q}' â†’ {len(chunks)} chunks\")\nrag_accuracy = sum(rag_scores) / len(rag_scores)\nprint(\"\\nðŸ§  RAG Retriever Coverage:\", rag_accuracy)\n\n# 3. Domain agent quality + latency\nagent_tests = {\n    \"PAYMENT_AGENT\": \"Payment deducted but no confirmation\",\n    \"REFUND_AGENT\": \"Refund not received after 5 days\",\n    \"ORDER_AGENT\": \"Where is my order?\",\n    \"TECH_AGENT\": \"App keeps crashing\",\n    \"ACCOUNT_AGENT\": \"How to delete my account?\",\n}\nagent_scores = []\nlatencies = []\nfor agent, query in agent_tests.items():\n    fn = AGENT_FN_MAP[agent]\n    t0 = time.time()\n    out = fn(query)\n    t1 = time.time()\n    lat = t1 - t0\n    latencies.append(lat)\n    # normalize\n    if isinstance(out, dict):\n        answer_text = out.get(\"content\", \"\").lower()\n    else:\n        answer_text = str(out).lower()\n    quality = 1 if any(k in answer_text for k in [\"refund\",\"order\",\"account\",\"crash\",\"payment\"]) else 0\n    print(f\"\\nAgent={agent}, Latency={lat:.2f}s\\nOutput = {answer_text[:200]} ...\")\n    agent_scores.append(quality)\nagent_quality = sum(agent_scores) / len(agent_scores)\nprint(\"\\nðŸ¤– Domain Agent Response Quality:\", agent_quality)\n\n# 4. A2A routing\nprint(\"\\nðŸ›° Testing A2A routing...\")\na2a_out = process_user_query(\"Payment deducted and refund not received after 5 days\", user_id=\"test_a2a\")\na2a_correct = (a2a_out[\"initial_agent\"] == \"PAYMENT_AGENT\" and a2a_out[\"final_agent\"] == \"REFUND_AGENT\")\nprint(\"A2A chain:\", a2a_out[\"a2a_trace\"])\nprint(\"A2A Correct:\", a2a_correct)\n\n# 5. Hallucination test\nprint(\"\\nðŸ’­ Hallucination Test...\")\nhallucination_queries = [\"Who is the CEO of our company?\", \"Tell me the interest rate on home loans\"]\nhall_scores = []\nfor q in hallucination_queries:\n    out = process_user_query(q, user_id=\"halluc_test\")\n    ans = out[\"answer\"].lower()\n    safe = any(phrase in ans for phrase in [\"i do not\", \"not available\", \"please provide\", \"i'm not sure\"])\n    hall_scores.append(1 if safe else 0)\nhall_score = sum(hall_scores) / len(hall_scores)\nprint(\"Hallucination score:\", hall_score)\n\n# 6. E2E latency\nprint(\"\\nðŸš¦ Running end-to-end evaluation...\")\ne2e_queries = [\"Payment deducted but no confirmation\", \"Where is my order?\", \"Refund not received\"]\nlat_list = []\nfor q in e2e_queries:\n    t0 = time.time()\n    out = process_user_query(q, user_id=\"eval_e2e\")\n    lat_list.append(time.time() - t0)\n    print(f\"Query '{q}' â†’ {lat_list[-1]:.2f}s\")\navg_latency = mean(lat_list)\nprint(\"\\nâš¡ Average E2E Latency:\", avg_latency)\n\n# Final scorecard\nprint(\"\\n========================\")\nprint(\"ðŸ“Š FINAL AGENT SCORECARD\")\nprint(\"========================\")\nprint(f\"Intent Accuracy:        {intent_accuracy:.2f}\")\nprint(f\"Retriever Coverage:     {rag_accuracy:.2f}\")\nprint(f\"Agent Quality:          {agent_quality:.2f}\")\nprint(f\"A2A Correct:            {a2a_correct}\")\nprint(f\"Hallucination Safety:   {hall_score:.2f}\")\nprint(f\"E2E Latency:            {avg_latency:.2f} sec\")\nprint(\"\\nâœ… Evaluation complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:00:49.039380Z","iopub.execute_input":"2025-11-29T12:00:49.039795Z","iopub.status.idle":"2025-11-29T12:01:00.938626Z","shell.execute_reply.started":"2025-11-29T12:00:49.039763Z","shell.execute_reply":"2025-11-29T12:01:00.937336Z"}},"outputs":[{"name":"stdout","text":"ðŸ“Œ Starting Agent Evaluation Suite...\n\nTest: Payment deducted but no confirmation â†’ predicted=PAYMENT_ISSUE, expected=PAYMENT_ISSUE\nTest: I want to delete my account â†’ predicted=ACCOUNT_ISSUE, expected=ACCOUNT_ISSUE\nTest: Where is my order? â†’ predicted=ORDER_STATUS, expected=ORDER_STATUS\nTest: App keeps crashing â†’ predicted=TECH_SUPPORT, expected=TECH_SUPPORT\nTest: I didnâ€™t receive my refund â†’ predicted=PAYMENT_ISSUE, expected=REFUND\n\nðŸŽ¯ Intent Classifier Accuracy: 0.8\nRAG for 'payment deducted' â†’ 2 chunks\nRAG for 'refund delayed' â†’ 2 chunks\nRAG for 'order pending' â†’ 2 chunks\nRAG for 'app crash' â†’ 2 chunks\n\nðŸ§  RAG Retriever Coverage: 1.0\n\nAgent=PAYMENT_AGENT, Latency=0.86s\nOutput = please wait 5 minutes. if you still don't have confirmation, please check your bank. a refund will be processed automatically if the payment fails. ...\n\nAgent=REFUND_AGENT, Latency=0.87s\nOutput = please provide your transaction id so i can investigate this for you. ...\n\nAgent=ORDER_AGENT, Latency=0.72s\nOutput = your order is pending. this means it's waiting for payment gateway confirmation. ...\n\nAgent=TECH_AGENT, Latency=0.84s\nOutput = clear cache, update app, restart device, or reinstall if needed. ...\n\nAgent=ACCOUNT_AGENT, Latency=0.78s\nOutput = to delete your account, please contact support with your reason. account deletion is irreversible. ...\n\nðŸ¤– Domain Agent Response Quality: 0.6\n\nðŸ›° Testing A2A routing...\nA2A chain: [{'hop': 1, 'from': 'PAYMENT_AGENT', 'to': 'REFUND_AGENT', 'content': 'Payment agent detected refund request.'}]\nA2A Correct: True\n\nðŸ’­ Hallucination Test...\nHallucination score: 0.0\n\nðŸš¦ Running end-to-end evaluation...\nQuery 'Payment deducted but no confirmation' â†’ 1.21s\nQuery 'Where is my order?' â†’ 1.24s\nQuery 'Refund not received' â†’ 1.08s\n\nâš¡ Average E2E Latency: 1.1772504647572835\n\n========================\nðŸ“Š FINAL AGENT SCORECARD\n========================\nIntent Accuracy:        0.80\nRetriever Coverage:     1.00\nAgent Quality:          0.60\nA2A Correct:            True\nHallucination Safety:   0.00\nE2E Latency:            1.18 sec\n\nâœ… Evaluation complete.\n","output_type":"stream"}],"execution_count":17}]}